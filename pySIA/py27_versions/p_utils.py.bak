# -*- coding: utf-8 -*-
"""
Created on Thu Nov  5 18:45:22 2015

@author: phil
"""

import os
import string
import h5py
import numpy as np
from scipy.optimize import curve_fit
import scipy.ndimage
from PIL import Image
from matplotlib import pyplot as plt
try:
    import cPickle as pickle
except:
    import pickle
import sys
from scipy import io
from skimage.transform import resize
'''TODO: add padding function'''


    
def userQuery(string,type,default,mode='user'):
    #modes are 'user': for user input, or 'default' automatically puts user input    
    
    
    if mode == 'user':
        if type == 'str':
            inpt = raw_input(string + '[' + default +']? ')
            if inpt == '':
                output = default
            else:
                output = inpt
        elif type == 'int':
            inpt = raw_input(string + '[' + str(default) +']? ')
            if inpt == '':
                output = default
            else:
                output = int(inpt)
        elif type == 'float':
            inpt = raw_input(string + '[' + str(default) +']? ')
            if inpt == '':
                output = default
            else:
                output = float(inpt)
        elif type == 'list':
            inpt = raw_input(string + '[ ' + str(default) +' ]? (list inputs seperated by spaces) ')
            if inpt == '':
                output = default
            else:
                output = map(int, inpt.split())
    else:
        output = default

    
    return output

def optDef(optionKey,optionDict,defaultVal):
    if optionKey in optionDict:
        optionVal  = optionDict[optionKey]
    else:        
        optionVal = defaultVal        
    return optionVal
    

def load_shared_data(data_set):
    import theano
    #puts the data onto the gpu
    shared_set = theano.shared(np.asarray(data_set,
                                               dtype=theano.config.floatX),borrow=True)
    return shared_set
    
def vaf(y_test,y_pred):
    cc  = np.corrcoef(y_test,y_pred)
    vaf = np.square(cc[0,1])*100
    return vaf
def powerLaw(x, a, b):
        y = a*(x**b)
        return y
def powerLawFit(y_true,y_pred):
    #need to testThis
    p_opt,p_cov =curve_fit(powerLaw, np.float64(y_pred), np.float64(y_true),p0=[0.5,1.0])
    return p_opt
    
def siaNLFit(y_true,y_pred):
    #with SIA, we use a half wave rectifier NL then we fit a power law
    try:
        y_pred = np.maximum(0,y_pred)
        p_opt = powerLawFit(y_true,y_pred)
    except RuntimeError:
        print('error in siaNLFit')
        p_opt = [1.0,1.0]
    return p_opt
def siaNLPredict(y_pred,p_opt):
    y_pred =  np.maximum(0,y_pred)
    y_NL = powerLaw(y_pred,p_opt[0],p_opt[1])
    return y_NL
    
def dataPadd():
    return
def minMaxScale(x):
    minVal = np.min(x)
    maxVal =  np.max(x)
    x_std = (x - minVal) / (maxVal- minVal)
    return x_std


def weightsToImage(weights):
    wShape = np.shape(np.squeeze(weights))
    wSize = np.size(weights)
    colWeights = np.reshape(weights,(wSize,))
    scaledWeights = minMaxScale(colWeights)
    newWeights = np.reshape(scaledWeights,wShape)
    img = Image.fromarray(newWeights*255)
    img = img.resize((480,480))
    img.show()
    
    return
def plotMapWeights(weights,n_kern,nLags,transpose=True):
    #weights are 1-d
    maxWeight = np.max(np.abs(weights))
    frameSize = (len(weights)/nLags)/n_kern
    kernSize = np.sqrt(frameSize)
    weights = np.reshape(weights,(nLags,n_kern,kernSize,kernSize))
    
    f, axarr = plt.subplots(n_kern, nLags)
    f.subplots_adjust(wspace=0.1)
    if n_kern == 1:
        axarr = np.expand_dims(axarr,axis=0)
    if nLags == 1:
        axarr = np.expand_dims(axarr,axis=1)
    
    for kern in range(n_kern):
        for lag in range(nLags):
            if transpose == True:
#                frame = np.rot90(weights[:,:,kern,lag],2)
                frame = np.transpose(weights[lag,kern,:,:])
            else:
                frame = weights[lag,kern,:,:]
                

            axarr[kern,lag]  .imshow(frame,cmap='gray',interpolation='none',clim=(-maxWeight,maxWeight))
            
            axarr[kern,lag]  .get_xaxis().set_visible(False)
            axarr[kern,lag]  .get_yaxis().set_visible(False)

        
    plt.show()
    return
def plotFilterWeights(weights,n_kern,transpose=True):
    #weights are given as an array of filters (each (n_kern,1,kernSize,kernSize))
    maxWeight = np.max(np.abs(weights))
    nLags = len(weights)
    f, axarr = plt.subplots(n_kern, nLags)
    f.subplots_adjust(wspace=0.1)
    if n_kern == 1:
        axarr = np.expand_dims(axarr,axis=0)
    if nLags == 1:
        axarr = np.expand_dims(axarr,axis=1)
        
    for lag in range(nLags):
        for kern in range(n_kern):
            if transpose == True:
                frame = np.rot90(np.transpose(weights[lag][kern,0,:,:]),2)
            else:
                frame = weights[lag][kern,0,:,:]
                

            axarr[kern,lag]  .imshow(frame,cmap='gray',interpolation='none',clim=(-maxWeight,maxWeight))
            
            axarr[kern,lag]  .get_xaxis().set_visible(False)
            axarr[kern,lag]  .get_yaxis().set_visible(False)

            
    
    plt.show()
    return
def plotReconstruction(mapWeights,filterWeights,kern_size =30,downsamp=3,mapLags=1,n_kern=1,transpose=True,plotFlag=True):
    
    frameSize = (len(mapWeights)/mapLags)/n_kern
    downsampleFrame_size = np.sqrt(frameSize)
    filterSize = np.shape(filterWeights[0])[3]
    mapWeights = np.reshape(mapWeights,(mapLags,n_kern,downsampleFrame_size,downsampleFrame_size))
    
    filterLags = len(filterWeights)
    if mapLags < filterLags:
        mapWeights = np.tile(mapWeights,(filterLags,1,1,1))
    
    reconFilter = np.zeros((filterLags,n_kern,kern_size,kern_size))    
    

    
    #recreate the filter
    for lag in range(filterLags):
        for kern in range(n_kern):
            if transpose == True:
#                frame = np.rot90(weights[:,:,kern,lag],2)
                mapFrame = np.transpose(mapWeights[lag,kern,:,:])
                filterFrame = np.rot90(np.transpose(filterWeights[lag][kern,0,:,:]),2)

            else:
                mapFrame = mapWeights[lag,kern,:,:]
                filterFrame = filterWeights[lag][kern,0,:,:]
                

            for x_idx in range(kern_size-filterSize+1):
                x_mapIdx = x_idx/downsamp
                for y_idx in range(kern_size-filterSize+1):
                    y_mapIdx = y_idx/downsamp
                    
                    if y_mapIdx < downsampleFrame_size and x_mapIdx < downsampleFrame_size:
                        reconFilter[lag,kern,y_idx:y_idx+filterSize,x_idx:x_idx+filterSize] = reconFilter[lag,kern,y_idx:y_idx+filterSize,x_idx:x_idx+filterSize] + filterFrame*mapFrame[y_mapIdx,x_mapIdx]
                
    #get the max weight, then plot 
    if plotFlag:
        maxWeight = np.max(np.abs(reconFilter))
        f, axarr = plt.subplots(n_kern, filterLags)
        f.subplots_adjust(wspace=0.1)
        if n_kern == 1:
            axarr = np.expand_dims(axarr,axis=0)
        if filterLags == 1:
            axarr = np.expand_dims(axarr,axis=1)
            
            
        for lag in range(filterLags):
            for kern in range(n_kern): 
                frame = reconFilter[lag,kern,:,:]
                axarr[kern,lag]  .imshow(frame,cmap='gray',interpolation='none',clim=(-maxWeight,maxWeight))
                
                axarr[kern,lag]  .get_xaxis().set_visible(False)
                axarr[kern,lag]  .get_yaxis().set_visible(False)
                
        
    
    return reconFilter
def upsample(mapWeights,filterSize,kern_size =30,downsamp=3,mapLags=1,n_kern=1,transpose=True):
    #assumes 1 kern for now!!!!!!!!!!!!!
    #purely upsampling, if you want to reverse the averaging, divide the output of this function by the square of downsamp
    frameSize = (len(mapWeights))
    downsampleFrame_size = np.int(np.sqrt(frameSize))

    mapWeights = np.reshape(mapWeights,(downsampleFrame_size,downsampleFrame_size))
    upMap = np.zeros((kern_size,kern_size))
    
    
    for x_idx in range(kern_size):
        x_mapIdx = x_idx/downsamp
        for y_idx in range(kern_size):
            y_mapIdx = y_idx/downsamp
            
            if y_mapIdx < downsampleFrame_size and x_mapIdx < downsampleFrame_size:
                upMap[y_idx:y_idx+filterSize,x_idx:x_idx+filterSize] = mapWeights[y_mapIdx,x_mapIdx]
        
    return np.reshape(upMap,(np.square(kern_size),1))
    
def fftDeconvolve(star, psf):
    from scipy import fftpack
    
    newImage = np.zeros((30,30))
    newImage[9:21,9:21] = psf
    
    psf= newImage
    star_fft = fftpack.fftshift(fftpack.fftn(star))
    psf_fft = fftpack.fftshift(fftpack.fftn(psf))
    return np.real(fftpack.fftshift(fftpack.ifftn(fftpack.ifftshift(star_fft/psf_fft))))    
    
def deconvolveWeights(mapWeights,filterWeights,mapResize=(30,30)):
    from skimage import restoration

#    deconvolved = restoration.richardson_lucy(mapWeights,filterWeights,iterations = 50)
    deconvolved = restoration.unsupervised_wiener(mapWeights,filterWeights)
    return deconvolved

def dataDelay(stim,trialSize,delay =[0]):
    ''' for every time point, new stim is the set of previous frames (delay ==0 is the current frame)
    Inputs: stim(m,n) array, m is the features, m is the examples
            trialSize, size of each trial, if you need a frame before the trial begins, it will be a zero-filled frame
            delay, array of delays to use. each delay corresponds to a previous input, ex delay = range(8), use all up to 7 preceding frames, 
            if delay = [0], new stim will be the same 
            if delay = [2], new stim will use only the stimulus from 2 frames ago'''
            
    stimSize = np.size(stim,axis=0)
    splitIndices = np.arange(0,stimSize-trialSize,trialSize)+trialSize
    splitList = np.split(stim,splitIndices,axis=0)
    

    #fill stim with zeros, to prepare for adding delays
    stim = np.zeros((stimSize,np.size(delay)*np.size(stim,axis=1)))
    for trialNum in range(len(splitList)):
        trial = splitList[trialNum]
        for frameNum in range(np.size(trial,axis=0)):
            stimFrame = []
            for k in delay:
                delayNum  = frameNum -k
                
                if delayNum < 0:
                    delayFrame = np.zeros(np.shape(trial[delayNum,:]))
                else:
                    delayFrame = trial[delayNum,:]  
                        
                stimFrame = np.concatenate((stimFrame,delayFrame))
            stim[frameNum+trialNum*trialSize,:] = stimFrame
    
    return stim
    
def dataDelayAsList(stim,numFrames):
    stim = np.split(stim,numFrames,axis= 1)
    stim = np.dstack(stim)
    stim = np.swapaxes(stim,1,2)
    return stim
def dataDelayAsStack(stim,numFrames):
    stim = np.split(stim,numFrames,axis= 1)
    stim = np.dstack(stim)
    stim = np.swapaxes(stim,1,2)
    return stim    
def getEstRegPred(resp):
    #assumes that trial lengths are all 375
    estIdx = int((2.0/3.0)*len(resp))

    regIdx = int((5.0/6.0)*len(resp))
    predIdx = len(resp)
    return estIdx,regIdx,predIdx
    
def getPaths():
    pathname = os.path.dirname(sys.argv[0])
    pySIALocation =  str(os.path.abspath(pathname))
    
    codeLocationSep = string.split(pySIALocation,os.path.sep)
    pyCodeLocation =  string.join(codeLocationSep[:-1],os.path.sep)
    codeLocationSep = string.split(pyCodeLocation,os.path.sep)
    SIALocation = string.join(codeLocationSep[:-2],os.path.sep)
    DATAlocation = string.join((SIALocation,'DATA'),os.path.sep)
    STRF_MOVIESlocation = string.join((SIALocation,'STRF_MOVIES'),os.path.sep)
    channelLocation = string.join((SIALocation,'channelScripts'),os.path.sep)
    AllNeuronLocation = string.join((channelLocation,'AllNeurons'),os.path.sep)
    
      

    os.chdir(SIALocation)
    

    os.chdir('MOVIES')
    MOVIESLocation = os.getcwd()
    
    pathStruct = dict()
    pathStruct['pySIALocation'] = pySIALocation
    pathStruct['pyCodeLocation'] = pyCodeLocation
    pathStruct['SIALocation'] = SIALocation
    pathStruct['DATAlocation'] = DATAlocation
    pathStruct['channelLocation'] = channelLocation
    pathStruct['AllNeuronLocation'] = AllNeuronLocation
    pathStruct['STRF_MOVIESlocation'] = STRF_MOVIESlocation
    pathStruct['MOVIESLocation'] = MOVIESLocation
    os.chdir(pySIALocation)
    return pathStruct

def getStrfData(strfFile,dataFile ='H5201.022_1_Ch34',cropKern = 30):
    import re
    pathStruct = getPaths()
    
    #IMPORT THE RESPONSE
    dataFileName = dataFile
    dataFolder = string.join((pathStruct['DATAlocation'],dataFileName),os.path.sep)
    dataFileStrf = dataFolder + os.path.sep + dataFileName + '_resp.mat'
    
    
    resp =np.transpose(np.array(io.loadmat(dataFileStrf)['response']))
    
    
    #IMPORT THE STIM       
    channelStruct = io.loadmat(pathStruct['AllNeuronLocation'] + os.path.sep + dataFile +'.mat')
    moviePath = str(channelStruct['stim']['mv'][0][0][0]['pathName'][0][0]) #getting the movie name from the awful matlab-python conversion
    movieName = re.split("\\\\|/",moviePath)[-1] #need to account for // or \\||/
 
    
    
    
    if 'Crop' in strfFile:
        #TODO CHECK THE RESIZE FUNCTION
        #will force final stim will be 30x30xnumFrames
        #if the file was created on a windows file system, then we need to 

        print movieName        
        print cropKern
        
        
        cropFileName = dataFolder + os.path.sep + dataFile+'_'+ strfFile +'.mat'        
        print 'starting cropping'
        #will need to import create the full 480x480 movie, then crop and downsample
        try:
            with h5py.File(cropFileName,'r') as f:
                cropWin = np.asarray(f['cropWin'])

        except:
            f = io.loadmat(cropFileName)
            cropWin = f['cropWin'][0]
            
        if np.sum(cropWin) == 0:
            cropWin = np.asarray([0,480,0,480])    
        
        movieFolder = pathStruct['MOVIESLocation'] + os.path.sep +movieName
        os.chdir(movieFolder) 
        

        
        for i in np.arange(30) +1:
            print i
            if i <= 20:
                movieNumber = "%02d" %i
                movieFileName = movieName +'_0000_' +movieNumber+ '.mat'
            elif (i >20) and (i <=25):                
                movieNumber = "%02d" %(i-20)
                movieFileName = movieName +'_reg_0000_' +movieNumber+ '.mat'
                
            elif i >25:
                movieNumber = "%02d" %(i-25)
                movieFileName = movieName +'_pred_0000_' +movieNumber+ '.mat'

            currMovie =io.loadmat(movieFileName)['mvMovie']
            currMovie = currMovie[cropWin[2]:cropWin[3],cropWin[0]:cropWin[1],:]
            currMovie = resize(currMovie,(cropKern,cropKern))
            
            if i == 1:
                numFrames = np.size(currMovie,2)
                stim = np.zeros((cropKern,cropKern,30*numFrames)) 
            stim[:,:,numFrames*(i-1):numFrames*i] = currMovie
            
            
#        stim = np.reshape(stim,(np.square(cropKern),30*numFrames))   
#        stim = stim - np.mean(stim)
#        stim = np.transpose(np.transpose(stim)/ np.std(stim))  #!!!! should switch this so that it divides by the total std
#        stim = np.reshape(stim,(cropKern,cropKern,30*numFrames)) 
        
#        stim = stim - np.mean(stim)
#        stim = stim/ np.std(stim)
        

        
        stim = resize(stim,(cropKern,cropKern))
        stim = stim.transpose((1,0,2))   #tranpose the image (for some reason it gets inverted somehwere)
        stim = np.transpose(np.reshape(stim,(np.square(cropKern),30*numFrames)))
        
        
    else:
        #just import the necessary strfFile
        movieFolder = pathStruct['STRF_MOVIESlocation'] + os.path.sep +movieName
        os.chdir(movieFolder)   
        
        movieFile = movieName + '_' + strfFile + '.mat'
        
        with h5py.File(movieFile,'r') as f:
            stim = np.array(f['stimMovieAll'])
        
        


#    try:
#        with h5py.File(dataFileStrf,'r') as f:
#            resp = np.array(f['globDat']['resp'])
#            stim = np.array(f['globDat']['stim'])
#            
#            #need to reshape the stim and resp correctly
#            resp = np.transpose(resp)
#            stim = np.transpose(stim)
#            
#    except IOError:
#        import scipy.io as sio
#        resp = np.array(sio.loadmat(dataFileStrf)['globDat']['resp'][0][0])
#        stim = np.array(sio.loadmat(dataFileStrf)['globDat']['stim'][0][0])
    
    
    resp = np.squeeze(resp)
    
    #remove frame doubling
#    resp = np.mean(resp.reshape(-1, 2), axis=1)
#    stim = stim[::2,:]
    
    
    return stim,resp

def normalize(X,featureMin = 0 ,featureMax= 1):
    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
    X_scaled = X_std * (featureMax - featureMin) + featureMin
    return X_scaled
def standardize(X):
    Xmean = np.mean(X,axis=0)
    Xstd = np.std(X,axis=0)
    Xnorm = X - Xmean
    Xnorm = Xnorm / Xstd
    return Xnorm
  
if __name__ == '__main__':
#    stim,resp = getStrfData()
##    stim,resp = getStrfData('pComplex')
#    stim = dataDelay(stim,375,range(8))
#    stim = dataDelay(stim,375,range(1))
#    stim,resp = getStrfData('pSimple')
#    stim = dataDelay(stim,375,range(1))
#    
    mapWeights= pickle.load(open('mapWeights.pkl','r'))
    mapWeights = np.squeeze(mapWeights)
#    from scipy.misc import imresize    
#    mapWeights = imresize(mapWeights,(30,30),interp = 'bilinear')/255.0
    
    mapWeights = scipy.ndimage.zoom(mapWeights,30./9, order=1)
    filterWeights= pickle.load(open('filterWeights.pkl','r'))
#    deconvWeights = deconvolveWeights(mapWeights,filterWeights)
    deconvWeights = fftDeconvolve(mapWeights,filterWeights)
    weightsToImage(mapWeights)
    weightsToImage(deconvWeights)
    print('i')